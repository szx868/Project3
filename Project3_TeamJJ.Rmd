---
title: "Project3"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Science Skills

**Team Triple J:** Jered Ataky, Zhouxin Shi, Irene Jacob

## Project Overview:

In this project, our goal is to be able to answer to the question:

“Which are the most valued data science skills?” 

As it is a group work and each member living in different time zone, we have 
established a great way of communication, code sharing, and documentation to
enable us to be successful and efficient while working virtually together.
The tools used and data source explored are described below: 

## Tools:

We are using Github for code sharing, and Google Cloud Platform (GCP)
for data base and storage.
In the other, Slack and Microsoft Teams are used for communication as well
project documentation.

## Data Source:

The data set we are working can be found in the link below:

https://www.kaggle.com/elroyggj/indeed-dataset-data-scientistanalystengineer


We also have it loaded as csv in our Github repository for project development:

(https://github.com/szx868/Project3)


## ER Diagram
![](Entity_Relationship_Diagram.jpg "Image Title")

## Work process & responsabilities

```{r}

Process <- c('Data Collection', 'Database Storage & Structure ',
             'Data Transformation (Cleaning & Tidying data)',
             'Data Analysis', 'Visualization',
             'Review & Summary','Conclusion and Presentation')

Team <- c('Zhouxin', 'Zhouxin & Jered', 'Jered & Irene', 
          'Jered, Zhouxin, Irene', 'Irene', 'Jered',
          'Jered, Zhouxin, Irene')

df_team <- data.frame(Process, Team)

names(df_team) <- c('Process', 'Team Members')

df_team %>%
  kbl(caption = "Work Process & Responsabilities") %>%
  kable_material(c("striped", "hover")) %>%
  row_spec(0, color = "indigo")


```

## Approach


<style>
div.aquamarine { background-color:#7fffd4; border-radius: 10px; padding: 5px;}
</style>
<div class = "aquamarine">

We target a data set in kaggle and decided to work on.
First, we create a database in MySQL and link it to our GCP data base to
allow each one to access it locally and make any change if necessary.
we reside our data in both the GCP data base and storage we created.
At this point, we load the data from the cloud and start our analysis.


</div> \hfill\break


## Libraries

```{r}
library(tidyverse)
library(kableExtra)
library(rvest)
library(stringr)
library(xtable)
library(tm)
library(RMySQL)
library(PGRdup)
library(broom)
library(googleCloudStorageR)
library(readr)
library(cloudml)

```


## Load the data

```{r}

raw_data <-read.csv("https://storage.googleapis.com/triplej_project3/indeed_job_dataset7.csv")

```

```{r}
# data insight

glimpse(raw_data)


```



## Data Cleansing



### Clean skill

```{r}

# Remove bracket in skill

data_1 <- data %>%
  separate(Skill, c("Alt", "new_skills"), sep = "^[\\[]", remove = TRUE, convert = FALSE) %>%
  separate(new_skills, c("Skill", "Alt"), sep = "[\\]]$", remove = TRUE, convert = FALSE)

data_1 <- data_1[1:3]

```



### Clean description



## Data Transformation



## Data Analysis



## Futur Work




## Findings





